<!DOCTYPE html>
<!-- adapted from http://bayesiandeeplearning.org/ -->
<html class="mel_workshop"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<title>Embodied Multimodal Learning Workshop | ICLR 2021</title>
		<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
		<meta http-equiv="Pragma" content="no-cache">
		<meta http-equiv="Expires" content="0">
		<meta name="description" content="Embodied Multimodal Learning Workshop | ICLR 2021">
		<meta name="keywords" content="Embodied,Multimodal,Learning,Workshop,ICLR,2021">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" href="./EML_files/main.css">
		<meta property="og:title" content="Embodied Multimodal Learning (EML)| ICLR 2021">
		<meta property="og:type" content="website">
		<meta property="og:url" content="http://eml-workshop.org">
		<meta property="og:description" content="Embodied Multimodal Learning Workshop at ICLR 2021 (Virtual) â€” Saturday, May 8th, 2021">
	</head>
	<body data-gr-c-s-loaded="true" class="">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<h1>Embodied Multimodal Learning (EML)</h1>
						<h2><b>ICLR 2021 Workshop (Virtual)</b></h2>
						<h2>Saturday, May 8th, 2021</h2>
					</header>

				<!-- Nav -->
					<nav id="nav" class="">
						<ul>
							<li><a href="https://eml-workshop.github.io/#abstract" class="active">Abstract</a></li>
							<li><a href="https://eml-workshop.github.io/#speakers" class="">Invited Speakers</a></li>
							<li><a href="https://eml-workshop.github.io/#cfp" class="">Call for Papers</a></li>
							<li><a href="https://eml-workshop.github.io/#organizers" class="">Organizers</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<!-- Introduction -->
							<section id="abstract" class="main">
								<div>
									<div class="content">
										<header class="major">
											<center>
												<h2>Abstract</h2>
											</center>
										</header>
										<h3>Understanding scenes and events is inherently a multimodal experience. We perceive the world by looking, listening, touching, smelling, and tasting. Our sensory, perceptual, and cognitive abilities are inextricably tied to our physical being. These multisensory inputs drive our activities: we observe the surroundings to avoid obstacles, listen to audio sources to navigate to the sound location, use natural language to communicate with each other, or touch an object to sense its physical properties. Despite encouraging progress in embodied learning over the past two decades, there is still a large gap between embodied agents' perception and human perception. Humans have remarkable capabilities combining all our multisensory inputs. To close the gap, embodied agents should also be enabled to see, hear, touch, and interact with their surroundings in order to select the appropriate actions. However, today's learning algorithms primarily operate on a single modality. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals jointly. The goal of this workshop is to share recent progress and discuss current challenges on embodied learning with multiple modalities.<br><br>

										The EML workshop will bring together researchers in different subareas of embodied multimodal learning including computer vision, natural language processing, robotics, machine learning, and cognitive science to examine the challenges and opportunities emerging from the design of embodied agents that unify their multisensory inputs. We will also have a panel session on this topic to review the current state and identify the research infrastructure needed to enable a stronger collaboration between researchers working on different modalities.
										</h3>
									

									</div>
									<!-- <span class="image"></span> -->
								</div>
							</section>



							<section id="speakers" class="main">
								<div>
									<div class="content">
										<header class="major">
											<center>
												<h2>Invited Speakers</h2>
											</center>
										</header>
										<div class="row uniform" align="center">
											<div class="3u 12u$(small)">
											<a href="http://www.cs.cmu.edu/~abhinavg/" target="_blank" class="image">
												<span class="image fit">
													<img src="./EML_files/abhinav.jpg" alt="">
												</span>
												<h2>Abhinav Gupta<br> (CMU & FAIR) </h2>
											</a>
											</div>
					 						<div class="3u 12u$(small)">
												<a href="https://fh295.github.io/" target="_blank" class="image">
													<span class="image fit">
														<img src="./EML_files/felix.jpg" alt="">
													</span>
													<h2>Felix Hill<br> (DeepMind) </h2>
												</a>
											</div>
					 						<div class="3u 12u$(small)">
												<a href="http://www.csc.kth.se/~danik/" target="_blank" class="image">
													<span class="image fit">
														<img src="./EML_files/danica.jpg" alt="">
													</span>
													<h2>Danica Kragic<br> (KTH) </h2>
												</a>
											</div>
											<div class="3u 12u$(small)">
												<a href="https://people.eecs.berkeley.edu/~svlevine/" target="_blank" class="image">
													<span class="image fit">
														<img src="./EML_files/levine.jpg" alt="">
													</span>
													<h2>Sergey Levine<br> (UC Berkeley) </h2>
												</a>
											</div>
										</div>	
										<div class="row uniform" align="center">
					 						<div class="3u 12u$(small)">
												<a href="https://people.eecs.berkeley.edu/~malik/" target="_blank" class="image">
													<span class="image fit">
														<img src="./EML_files/malik.jpg" alt="">
													</span>
													<h2>Jitendra Malik<br> (UC Berkeley & FAIR) </h2>
												</a>
											</div>
					 						<div class="3u 12u$(small)">
												<a href="https://profiles.stanford.edu/silvio-savarese" target="_blank" class="image">
													<span class="image fit">
														<img src="./EML_files/silvio.jpg" alt="">
													</span>
													<h2>Silvio Savarese<br> (Stanford) </h2>
												</a>
											</div>
					 						<div class="3u 12u$(small)">
												<a href="https://psych.indiana.edu/directory/faculty/smith-linda.html" target="_blank" class="image">
													<span class="image fit">
														<img src="./EML_files/smith.jpg" alt="">
													</span>
													<h2>Linda Smith<br> (Indiana University) </h2>
												</a>
											</div>
									</div>											
								</div>
							</section>


							<section id="cfp" class="main">
								<div>
									<div class="content">
										<header class="major">
											<center>
											<h2>Call for Papers</h2>
											</center>
										</header>
										<h3>We invite submissions of 2-4 page extended abstracts in topics related to (but not limited to): 
										<ul>
												<li>audio-visual embodied learning</li>
												<li>touch sensing and embodied learning</li>
												<li>language/speech and embodied learning</li>
												<li>self-supervised/semi-supervised learning with multiple modalities</li>
												<li>multimodal reinforcement learning</li>
												<li>meta-learning with multiple modalities</li>
												<li>novel multimodal datasets/simulators/tasks for embodied agents</li>
												<li>combining multisensory inputs for robot perception</li>
												<li>bio-inspired approaches for multimodal perception</li>
										</ul>
											A submission should take the form of an extended abstract (2-4 pages long excluding references) in PDF format using the <a href="https://eml-workshop.github.io/ICLR2021_EML_Workshop.zip" target="_blank">ICLR style</a>. We will accept submissions of (1) papers that have not been previously published or accepted for publication in substantially similar form; (2) papers that have been published or accepted for publication in recent venues including journal, conference, and workshop; and (3) research proposals for future work with a focus on well-defined concepts and ideas. All submissions will be reviewed with double blind policy. Accepted extended abstracts will not appear in ICLR proceedings, and hence will not affect future publication of the work. We will publish all accepted extended abstracts on the workshop webpage.
										</h3>
										<br><br>
										<h2>Key Dates:
											<h3>
											<ul>
												<li>Extended abstract submission deadline: February 26th, 2021</li>
												<li>Notification to authors: March 26th, 2021</li>
												<li>Workshop date: May 8th, 2021</li>
											</ul>
											</h3>
										</h2>
									</div>
								</div>
							</section>

							<section id="organizers" class="main special">
								<div>
									<div class="content">
										<header class="major">
											<h2>Organizers</h2>
										</header>
										<div class="row uniform">
											<div class="2u 12u$(small)">
												<a href="https://www.cs.utexas.edu/~rhgao/" target="_blank" class="image">
													<span class="image fit">
														<img src="./EML_files/ruohan.jpg" alt="">
													</span>
													<h3>Ruohan Gao <br> (UT Austin)</h2>
												</a>
											</div>
											<div class="2u 12u$(small)">
												<a href="http://andrewowens.com/" target="_blank" class="image">
													<span class="image fit">
														<img src="./EML_files/andrew.jpg" alt="">
													</span>
													<h3>Andrew Owens <br> (UMich)</h2>
												</a>
											</div>
											<div class="2u 12u$(small)">
												<a href="https://www.seas.upenn.edu/~dineshj/" target="_blank" class="image">
													<span class="image fit">
														<img src="./EML_files/dinesh.jpg" alt="">
													</span>
													<h3>Dinesh Jayaraman<br> (UPenn)</h2>
												</a>
											</div>
											<div class="2u 12u$(small)">
												<a href="https://www.cs.utexas.edu/~yukez/" target="_blank" class="image">
													<span class="image fit">
														<img src="./EML_files/yuke.jpg" alt="">
													</span>
													<h3>Yuke Zhu<br> (UT Austin & Nvidia)</h2>
												</a>
											</div>
											<div class="2u 12u$(small)">
												<a href="https://jiajunwu.com/" target="_blank" class="image">
													<span class="image fit">
														<img src="./EML_files/jiajun.jpg" alt="">
													</span>
													<h3>Jiajun Wu <br> (Stanford)</h2>
												</a>
											</div>
											<div class="2u 12u$(small)">
												<a href="http://www.cs.utexas.edu/users/grauman/" target="_blank" class="image">
													<span class="image fit">
														<img src="./EML_files/grauman.jpg" alt="">
													</span>
													<h3>Kristen Grauman <br> (UT Austin & FAIR)</h2>
												</a>
											</div>
										</div>
										<br>
									</div>
								</div>
							</section>
					</div>

				<!-- Footer -->
					<footer id="footer">
						<p class="copyright">Website design adapted from <a href="http://bayesiandeeplearning.org/">Yarin Gal </a> and based on <a href="https://html5up.net/">HTML5 UP</a>.</p>
					</footer>

			</div>

		<!-- Scripts -->
			<script async="" src="./MEL_files/analytics.js"></script><script src="./EML_files/jquery.min.js"></script>
			<script src="./EML_files/jquery.scrollex.min.js"></script>
			<script src="./EML_files/jquery.scrolly.min.js"></script>
			<script src="./EML_files/skel.min.js"></script>
			<script src="./EML_files/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="./EML_files/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-JC1QVS8GW2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-JC1QVS8GW2');
</script>
	
</body></html>